{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size = 0.1, random_state = 42)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from functools import partial\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    n_inputs = x_train.shape[1]\n",
    "    n_hidden1 = 512\n",
    "    n_hidden2 = 256\n",
    "    n_hidden3 = 128\n",
    "    n_outputs = 2\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "    my_batch_norm_layer=partial(tf.layers.batch_normalization,training=training, momentum=0.9)\n",
    "  \n",
    "    X_drop = tf.layers.dropout(X, 0.2, training=training)\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        hidden1 = tf.layers.dense(X_drop,n_hidden1, kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "        bn1 = my_batch_norm_layer(hidden1)\n",
    "        bn1_act = tf.nn.elu(bn1)\n",
    "        hidden1_drop=tf.layers.dropout(bn1_act,rate=0.2,training=training)\n",
    "        \n",
    "        hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "        bn2 = my_batch_norm_layer(hidden2)\n",
    "        bn2_act = tf.nn.elu(bn2)\n",
    "        hidden2_drop=tf.layers.dropout(bn2_act,rate=0.2,training=training)\n",
    "        \n",
    "        hidden3 = tf.layers.dense(hidden2_drop, n_hidden3, kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "        bn3 = my_batch_norm_layer(hidden3)\n",
    "        bn3_act = tf.nn.elu(bn3)\n",
    "        hidden3_drop=tf.layers.dropout(bn3_act,rate=0.2,training=training)\n",
    "        #hidden4 = neuron_layer(hidden3, n_hidden4, name=\"hidden4\",\n",
    "        #                       activation=tf.nn.relu)\n",
    "        logits_before_bn = tf.layers.dense(hidden3_drop, n_outputs, activation=tf.nn.relu)\n",
    "        logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "    ratio = 0.08\n",
    "    class_weight =[ratio, 1.0 - ratio] \n",
    "    logits=logits*class_weight\n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate,momentum=0.9)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "   \n",
    "    with  tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) \n",
    "        \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    n_epochs = 2\n",
    "    batch_size = 100\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "            x_train = x_train.iloc[shuffle_indices]\n",
    "            y_train = y_train[shuffle_indices]\n",
    "            for iteration in range(len(y_train) // batch_size):\n",
    "                #X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                start = iteration * batch_size\n",
    "                X_batch = x_train[start:start + batch_size]\n",
    "                y_batch = y_train[start:start + batch_size]\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: x_test, y: y_test})\n",
    "            print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "            \n",
    "            # Check on the AUC\n",
    "            auc_train = roc_auc_score(y_batch,np.argmax(tf.nn.softmax(logits).eval(feed_dict={X: X_batch}),axis=1))\n",
    "            auc_test = roc_auc_score(y_test,np.argmax(tf.nn.softmax(logits).eval(feed_dict={X: x_test}),axis=1))\n",
    "            print(epoch, \"Train AUC:\", auc_train, \"Test AUC:\", auc_test)\n",
    "            print(np.argmax(tf.nn.softmax(logits).eval(feed_dict={X: X_batch}),axis=1))\n",
    "            print(y_batch)\n",
    "            #roc_auc_score(y_batch,np.argmax(tf.nn.softmax(logits).eval(feed_dict={X: X_batch})))\n",
    "        save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "        Z = tf.nn.softmax(logits).eval(feed_dict={X: X_test})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
